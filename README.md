# NFHL ETL Pipeline  

## Sandbox

This is a sandbox where I will be doing testing. 


**DONE** : 'E'â€”extract and 'T'-transform so far  
**NEXT** : 'Dockerize' these services + Deploy with Cloud Run


## Summary

- **E**: This repo can use the `Selenium WebDriver protocol`, configured for the Google Chrome web browser, to scrape the National Flood Hazard Layer (NFHL) from the [FEMA Map Service Center](https://msc.fema.gov/portal/advanceSearch#searchresultsanchor) (MSC) or make `API requests` from the [ArcGIS REST Services Directory](https://hazards.fema.gov/gis/nfhl/rest/services/public/NFHL/MapServer) for NFHL data.  
- **T**: Downloaded NFHL data is converted to `GeoJSON` format via the python library [`GDAL`](https://gdal.org/index.html).  
- **L**: `GeoJSON` files are then first uploaded to a Google Cloud Storage bucket for staging, then tranferred to a BigQuery dataset to further explore, clean and prepare the data for analysis.


## Run Pipeline

1. Navigate to your `~/Downloads` directory in terminal.
2. `git clone https://github.com/derekz3/NFHL_ETL.git pipeline`
3. `cd pipeline`
4. `chmod +x build.sh crawl.sh fema.sh`
5. `./build.sh` to build environment.
6. Enter password when prompted.
7. Method 1: `./crawl.sh` to implement web crawler. (Check `polygon.csv` for output)
8. Method 2: `./fema.sh` to implement FEMA API requests. (Check `out.json` for output)


## Update '`main`' Branch

1. git add -A
2. git commit -m "[*commit message*]"
3. git push origin main